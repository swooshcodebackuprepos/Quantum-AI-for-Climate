{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8014f74c-ada0-4f19-b211-2b76aa4d8c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xarray netcdf4\n",
    "# !pip install xarray netcdf4 requests\n",
    "# !pip install tensorflow\n",
    "# !pip install requests xarray pandas scikit-learn matplotlib streamlit\n",
    "# !pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84451d-a6a8-4603-a952-b04419e68fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Plan: Quantum and AI for Improved Typhoon Prediction in Taiwan\n",
    "# Project Overview\n",
    "# This project aims to leverage quantum computing and AI to enhance the accuracy and \n",
    "# prediction window of typhoon models. \n",
    "# The goal is to provide early warnings, improve trajectory predictions, and assess\n",
    "# the severity of storms more accurately, \n",
    "# giving authorities and citizens more time to prepare and respond.\n",
    "\n",
    "# Problem Statement\n",
    "# Current typhoon prediction models provide a 2-3 day heads-up and predict 1-2 days ahead after landfall. \n",
    "# The objective is to extend this prediction window to a week and improve \n",
    "# the accuracy of trajectory and severity forecasts.\n",
    "\n",
    "# Objectives\n",
    "# Develop a hybrid quantum-classical algorithm for typhoon prediction.\n",
    "# Utilize quantum simulations to enhance trajectory and severity predictions.\n",
    "# Apply AI techniques to analyze and interpret large-scale typhoon data.\n",
    "# Provide actionable insights and early warnings for disaster preparedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958d7df-e383-4235-bc9f-8ec5344c5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevance to Climate and Environmental Simulations: The project focuses on enhancing typhoon prediction, \n",
    "# directly related to climate and environmental simulations.\n",
    "# Application of Quantum Computing and AI: It leverages both quantum computing and AI to improve prediction accuracy \n",
    "# and extend the forecasting window.\n",
    "# High-Performance Modeling: It addresses the need for high-performance modeling and simulation, \n",
    "# a core interest of NNL.\n",
    "# Innovation and Research Focus: The project involves researching state-of-the-art techniques \n",
    "# and potentially contributing new findings, which aligns with NNL’s emphasis on pioneering research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d711d-5068-4a13-a59a-6976dab42b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methodology\n",
    "# 1. Literature Review:\n",
    "\n",
    "# Study existing typhoon prediction models and their limitations.\n",
    "# Research quantum computing algorithms relevant to weather simulations.\n",
    "# Investigate AI methods for large-scale data analysis and predictive modeling.\n",
    "# 2. Data Collection:\n",
    "\n",
    "# Source historical typhoon data and real-time weather data from reliable datasets (e.g., CWB, NASA, NOAA).\n",
    "# Preprocess and clean the data for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69705dc2-1d19-45ef-85ff-c32c18532d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inspect the columns of the merged_data DataFrame\n",
    "# print(merged_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87a61d03-f587-4c79-9b8a-228083048025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded aravg.mon.ocean.90S.90N.v5.0.0.202212.asc\n",
      "Downloaded ersst.v4.202001.nc\n",
      "Downloaded ersst.v4.202002.nc\n",
      "Downloaded avhrr-only-v2.20200101.nc\n",
      "Downloaded avhrr-only-v2.20200201.nc\n",
      "Downloaded 2020.csv.gz\n",
      "Downloaded 2021.csv.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Define the list of URLs for all datasets\n",
    "ersst_urls = [\n",
    "    \"https://www.ncei.noaa.gov/pub/data/cmb/ersst/v4/netcdf/ersst.v4.202001.nc\",\n",
    "    \"https://www.ncei.noaa.gov/pub/data/cmb/ersst/v4/netcdf/ersst.v4.202002.nc\",\n",
    "]\n",
    "\n",
    "avhrr_urls = [\n",
    "    \"https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2/access/avhrr-only/202001/avhrr-only-v2.20200101.nc\",\n",
    "    \"https://www.ncei.noaa.gov/data/sea-surface-temperature-optimum-interpolation/v2/access/avhrr-only/202002/avhrr-only-v2.20200201.nc\",\n",
    "]\n",
    "\n",
    "gst_url = \"https://www.ncei.noaa.gov/data/noaa-global-surface-temperature/v5/access/timeseries/aravg.mon.ocean.90S.90N.v5.0.0.202212.asc\"\n",
    "\n",
    "ghcn_urls = [\n",
    "    \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/2020.csv.gz\",\n",
    "    \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/2021.csv.gz\",\n",
    "]\n",
    "\n",
    "# Function to download and save data\n",
    "def download_data(url, folder):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    filename = url.split('/')[-1]\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    response = requests.get(url)\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "# Download datasets\n",
    "download_data(gst_url, 'gst_data')\n",
    "for url in ersst_urls:\n",
    "    download_data(url, 'ersst_v4_data')\n",
    "for url in avhrr_urls:\n",
    "    download_data(url, 'avhrr_data')\n",
    "for url in ghcn_urls:\n",
    "    download_data(url, 'ghcn_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510d643e-e651-4123-ac4a-712938ef01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load multiple NetCDF files into a single xarray Dataset\n",
    "def load_datasets(folder, ext='.nc'):\n",
    "    data_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(ext)]\n",
    "    datasets = [xr.open_dataset(file) for file in data_files]\n",
    "    combined_ds = xr.concat(datasets, dim='time')\n",
    "    return combined_ds\n",
    "\n",
    "# Load NetCDF datasets\n",
    "ersst_data = load_datasets('ersst_v4_data')\n",
    "avhrr_data = load_datasets('avhrr_data')\n",
    "\n",
    "# Load ASCII datasets (for GST)\n",
    "def load_ascii_data(file_path, delimiter=r\"\\s+\"):\n",
    "    data = pd.read_csv(file_path, delimiter=delimiter, header=None)\n",
    "    return data\n",
    "\n",
    "gst_data = load_ascii_data('gst_data/aravg.mon.ocean.90S.90N.v5.0.0.202212.asc')\n",
    "\n",
    "# Load CSV.gz datasets (for GHCN)\n",
    "def load_csv_data(folder):\n",
    "    data_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.csv.gz')]\n",
    "    data_list = []\n",
    "    for file in data_files:\n",
    "        data = pd.read_csv(file, compression='gzip', header=None)\n",
    "        data_list.append(data)\n",
    "    combined_data = pd.concat(data_list, axis=0)\n",
    "    return combined_data\n",
    "\n",
    "ghcn_data = load_csv_data('ghcn_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59fc7d94-7f4c-47f9-8a9d-85345a7e5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    return (data - data.mean()) / data.std()\n",
    "\n",
    "def extract_features(data, var_name):\n",
    "    df = data[var_name].to_dataframe().reset_index()\n",
    "    df[var_name] = normalize(df[var_name])\n",
    "    return df\n",
    "\n",
    "ersst_sst = extract_features(ersst_data, 'sst')\n",
    "ersst_ssta = extract_features(ersst_data, 'ssta')\n",
    "avhrr_sst = extract_features(avhrr_data, 'sst')\n",
    "avhrr_anom = extract_features(avhrr_data, 'anom')\n",
    "\n",
    "# Ensure 'zlev' is present in the DataFrames\n",
    "ersst_sst['zlev'] = 0\n",
    "ersst_ssta['zlev'] = 0\n",
    "if 'zlev' not in avhrr_sst.columns:\n",
    "    avhrr_sst['zlev'] = 0\n",
    "if 'zlev' not in avhrr_anom.columns:\n",
    "    avhrr_anom['zlev'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcfc5c4c-ff88-4eae-91ac-a6ec8c6fb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Function to resample AVHRR data to match ERSST resolution\n",
    "def resample_avhrr_data(avhrr_df, ersst_df):\n",
    "    avhrr_resampled = avhrr_df.groupby(['time', 'lat', 'lon']).mean().reset_index()\n",
    "    return avhrr_resampled\n",
    "\n",
    "avhrr_sst_resampled = resample_avhrr_data(avhrr_sst, ersst_sst)\n",
    "avhrr_anom_resampled = resample_avhrr_data(avhrr_anom, ersst_sst)\n",
    "\n",
    "# Prepare nearest neighbors model for lat/lon matching\n",
    "def match_nearest_neighbors(df1, df2, on_columns):\n",
    "    nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(df2[on_columns].dropna())\n",
    "    distances, indices = nn.kneighbors(df1[on_columns].dropna())\n",
    "    matched_df = df1.copy()\n",
    "    matched_df.loc[df1[on_columns].dropna().index, on_columns] = df2.iloc[indices.flatten()][on_columns].values\n",
    "    return matched_df\n",
    "\n",
    "matched_avhrr_sst = match_nearest_neighbors(avhrr_sst_resampled, ersst_sst, ['lat', 'lon'])\n",
    "matched_avhrr_anom = match_nearest_neighbors(avhrr_anom_resampled, ersst_sst, ['lat', 'lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf0c944-5f90-4f29-9dcb-1d5da9006892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame Columns:\n",
      "Index(['time', 'lev', 'lat', 'lon', 'sst', 'zlev', 'ssta', 'sst_avhrr_sst',\n",
      "       'anom'],\n",
      "      dtype='object')\n",
      "First 5 rows of merged_data:\n",
      "              time  lev   lat    lon       sst  zlev      ssta  sst_avhrr_sst  \\\n",
      "5692928 2020-01-01  0.0 -76.0  164.0 -1.273254     0 -0.330044      -1.310388   \n",
      "5692929 2020-01-01  0.0 -76.0  164.0 -1.273254     0 -0.330044      -1.310388   \n",
      "5692930 2020-01-01  0.0 -76.0  164.0 -1.273254     0 -0.330044      -1.310388   \n",
      "5692931 2020-01-01  0.0 -76.0  164.0 -1.273254     0 -0.330044      -1.310388   \n",
      "5692932 2020-01-01  0.0 -76.0  164.0 -1.273254     0 -0.330044      -1.310388   \n",
      "\n",
      "             anom  \n",
      "5692928 -0.838026  \n",
      "5692929 -0.838026  \n",
      "5692930 -0.814510  \n",
      "5692931 -0.755719  \n",
      "5692932 -0.673412  \n",
      "Shape of X: (86654622, 4)\n",
      "Shape of y: (86654622,)\n",
      "First 5 rows of X:\n",
      "[[-1.2732539  -0.33004436 -1.310388   -0.8380261 ]\n",
      " [-1.2732539  -0.33004436 -1.310388   -0.8380261 ]\n",
      " [-1.2732539  -0.33004436 -1.310388   -0.8145098 ]\n",
      " [-1.2732539  -0.33004436 -1.310388   -0.7557192 ]\n",
      " [-1.2732539  -0.33004436 -1.310388   -0.6734122 ]]\n",
      "First 5 rows of y:\n",
      "[-1.2732539 -1.2732539 -1.2732539 -1.2732539 -1.2732539]\n"
     ]
    }
   ],
   "source": [
    "# Merge all datasets into a single DataFrame for modeling\n",
    "merged_data = ersst_sst.merge(ersst_ssta, on=['time', 'lat', 'lon', 'lev', 'zlev'], suffixes=('_sst', '_ssta'), how='inner')\n",
    "merged_data = merged_data.merge(matched_avhrr_sst, on=['time', 'lat', 'lon', 'zlev'], suffixes=('', '_avhrr_sst'), how='inner')\n",
    "merged_data = merged_data.merge(matched_avhrr_anom, on=['time', 'lat', 'lon', 'zlev'], suffixes=('', '_avhrr_anom'), how='inner')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "merged_data.dropna(inplace=True)\n",
    "\n",
    "# Check the columns and a sample of the merged_data DataFrame\n",
    "print(\"Merged DataFrame Columns:\")\n",
    "print(merged_data.columns)\n",
    "print(\"First 5 rows of merged_data:\")\n",
    "print(merged_data.head())\n",
    "\n",
    "# Define features and target for LSTM\n",
    "feature_cols = ['sst', 'ssta', 'sst_avhrr_sst', 'anom']\n",
    "target_col = 'sst'\n",
    "\n",
    "X = merged_data[feature_cols].values\n",
    "y = merged_data[target_col].values\n",
    "\n",
    "# Check the shapes of X and y\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# Inspect first few rows of X and y to ensure data is not empty\n",
    "print(f\"First 5 rows of X:\\n{X[:5]}\")\n",
    "print(f\"First 5 rows of y:\\n{y[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "934ff28f-17fc-4250-b819-13d9964e7009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_lstm: (86654612, 10, 4)\n",
      "Shape of y_lstm: (86654612,)\n",
      "Shape of X_train: (69323689, 10, 4)\n",
      "Shape of X_test: (17330923, 10, 4)\n",
      "Shape of y_train: (69323689,)\n",
      "Shape of y_test: (17330923,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # Ensure numpy is imported\n",
    "\n",
    "# Prepare LSTM data\n",
    "def prepare_lstm_data(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:i+time_steps])\n",
    "        ys.append(y[i+time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_lstm, y_lstm = prepare_lstm_data(X, y)\n",
    "\n",
    "# Check the shapes of X_lstm and y_lstm\n",
    "print(f\"Shape of X_lstm: {X_lstm.shape}\")\n",
    "print(f\"Shape of y_lstm: {y_lstm.shape}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split  # Ensure train_test_split is imported\n",
    "\n",
    "if X_lstm.shape[0] > 0:  # Ensure there is data to split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)\n",
    "    print(f\"Shape of X_train: {X_train.shape}\")\n",
    "    print(f\"Shape of X_test: {X_test.shape}\")\n",
    "    print(f\"Shape of y_train: {y_train.shape}\")\n",
    "    print(f\"Shape of y_test: {y_test.shape}\")\n",
    "else:\n",
    "    print(\"No data available for splitting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5f6603-64c9-4f26-9334-dee0cbcc703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1733093/1733093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8382s\u001b[0m 5ms/step - loss: 6.1081e-05 - val_loss: 1.1038e-06\n",
      "Epoch 2/20\n",
      "\u001b[1m1733093/1733093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8299s\u001b[0m 5ms/step - loss: 1.3777e-06 - val_loss: 1.0243e-06\n",
      "Epoch 3/20\n",
      "\u001b[1m1733093/1733093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8151s\u001b[0m 5ms/step - loss: 1.3532e-06 - val_loss: 1.0535e-06\n",
      "Epoch 4/20\n",
      "\u001b[1m1733093/1733093\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8425s\u001b[0m 5ms/step - loss: 1.1017e-06 - val_loss: 1.0277e-06\n",
      "Epoch 5/20\n",
      "\u001b[1m 555167/1733093\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46:17\u001b[0m 5ms/step - loss: 1.3414e-06"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Ensure X_train is defined and has the correct shape before running this block\n",
    "if 'X_train' in globals() and X_train.shape[0] > 0:\n",
    "    # Define LSTM model\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        LSTM(50, return_sequences=True),\n",
    "        LSTM(50),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "    # Save the model in the native Keras format\n",
    "    model.save('my_model.keras')\n",
    "\n",
    "    # Plot predictions vs actual values\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(y_test, label='Actual')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Training data is not defined or has incorrect shape.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c576e-c824-47c3-ae41-ec3eae1cc257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_real_time(new_data):\n",
    "    # Process new_data to match the format of X_train\n",
    "    # new_data should be a DataFrame or array similar to merged_data\n",
    "    X_new = new_data[feature_cols].values.reshape((1, new_data.shape[0], len(feature_cols)))\n",
    "    prediction = model.predict(X_new)\n",
    "    return prediction\n",
    "\n",
    "# Example usage\n",
    "# new_data = pd.DataFrame([...])  # New data in the same format as merged_data\n",
    "# print(predict_real_time(new_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f6f75b-7755-4e64-8b9b-567395cd9f24",
   "metadata": {},
   "source": [
    "#### import streamlit as st\n",
    "\n",
    "st.title('Typhoon Prediction Dashboard')\n",
    "\n",
    "# Ensure y_test and y_pred are defined before plotting\n",
    "if 'y_test' in globals() and 'y_pred' in globals():\n",
    "    st.line_chart({'Actual': y_test, 'Predicted': y_pred})\n",
    "else:\n",
    "    st.write(\"y_test and y_pred are not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8fd2e9-92fa-4109-9e8b-c5f498f50915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit as st\n",
    "\n",
    "# st.title('Typhoon Prediction Dashboard')\n",
    "\n",
    "# # Ensure y_test and y_pred are defined before plotting\n",
    "# if 'y_test' in globals() and 'y_pred' in globals():\n",
    "#     st.line_chart({'Actual': y_test, 'Predicted': y_pred})\n",
    "# else:\n",
    "#     st.write(\"y_test and y_pred are not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc835f-a90f-4dd3-ad48-44d25180df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit as st\n",
    "\n",
    "# st.title('Typhoon Prediction Dashboard')\n",
    "\n",
    "# # Ensure y_test and y_pred are defined before plotting\n",
    "# if 'y_test' in globals() and 'y_pred' in globals():\n",
    "#     st.line_chart({'Actual': y_test[:1000], 'Predicted': y_pred[:1000]})  # Plot only a subset for better performance\n",
    "# else:\n",
    "#     st.write(\"y_test and y_pred are not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed611c65-70af-4320-9b89-0c39643b60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# # Adjusting the LSTM units and batch size\n",
    "# model = Sequential()\n",
    "# model.add(tf.keras.Input(shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# model.add(LSTM(100, return_sequences=True))  # Adjusted LSTM units\n",
    "# model.add(LSTM(100))  # Adjusted LSTM units\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# # Train the model with different batch size\n",
    "# model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81ec16-d34f-4784-a67e-7a790317ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_real_time(new_data):\n",
    "#     # Process new_data to match the format of X_train\n",
    "#     # new_data should be a DataFrame or array similar to merged_data\n",
    "#     X_new = new_data[feature_cols].values.reshape((1, new_data.shape[0], len(feature_cols)))\n",
    "#     prediction = model.predict(X_new)\n",
    "#     return prediction\n",
    "\n",
    "# # Example usage\n",
    "# # new_data = pd.DataFrame([...])  # New data in the same format as merged_data\n",
    "# # print(predict_real_time(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bc6d1-9a07-4723-ab8e-d34000e7c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit as st\n",
    "\n",
    "# st.title('Typhoon Prediction Dashboard')\n",
    "\n",
    "# # Ensure y_test and y_pred are defined before plotting\n",
    "# if 'y_test' in globals() and 'y_pred' in globals():\n",
    "#     st.line_chart({'Actual': y_test, 'Predicted': y_pred})\n",
    "# else:\n",
    "#     st.write(\"y_test and y_pred are not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fd0b85-9e40-4bd0-b20b-1950674ff10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('stop here wed 24 2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d05958a-4161-4e47-b3d9-c34c9aefd5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the sea surface temperature anomalies over time to identify patterns that \n",
    "# could be indicative of typhoon formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27748a37-f215-4862-9130-159716b305ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Plot sea surface temperature anomalies\n",
    "# ssta_data.mean(dim='time').plot(cmap='coolwarm')\n",
    "# plt.title('Average Sea Surface Temperature Anomalies')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2ce06-4543-442f-9cde-5608abcfbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Predictive Model\n",
    "# To build a predictive model for typhoon prediction, we'll use historical typhoon data \n",
    "# and combine it with the preprocessed climate data. For this example, \n",
    "# we'll use a simple machine learning model (e.g., Random Forest) to predict \n",
    "# the occurrence of a typhoon based on sea surface temperature anomalies.\n",
    "\n",
    "# Load Historical Typhoon Data:\n",
    "\n",
    "# Obtain historical typhoon data from reliable sources such as NOAA or other meteorological databases.\n",
    "# Preprocess the typhoon data to align with the climate data (time, location, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c63fdf-3896-4f76-a504-bba0f9bdd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps:\n",
    "# Enhance the Model:\n",
    "\n",
    "# Experiment with different machine learning algorithms (e.g., LSTM, Gradient Boosting).\n",
    "# Implement feature engineering to extract more relevant features from the datasets.\n",
    "# Integrate Quantum Computing:\n",
    "\n",
    "# Use quantum algorithms to optimize the predictive model.\n",
    "# Leverage quantum machine learning techniques to enhance the model's performance.\n",
    "# Deploy the Model:\n",
    "\n",
    "# Create a real-time prediction system to provide early warnings for typhoons.\n",
    "# Develop a visualization dashboard to monitor predictions and alert relevant authorities.\n",
    "# By following these steps, you can develop a robust model for improved typhoon prediction using \n",
    "# quantum computing and AI, potentially providing more accurate and timely warnings to \n",
    "# enhance disaster preparedness and response in Taiwan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52bcabc-26c1-47db-ad9a-2207a4210e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance the Model:\n",
    "# Experiment with Different Machine Learning Algorithms:\n",
    "\n",
    "# LSTM (Long Short-Term Memory): Use LSTM networks to capture temporal dependencies in the climate data, \n",
    "# which can improve the prediction of typhoon trajectories and intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a7b458-44a7-4b2c-ab0d-3452608532b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Feature Engineering:\n",
    "\n",
    "# Extract Additional Features:\n",
    "# Sea surface temperature gradients\n",
    "# Atmospheric pressure differences\n",
    "# Historical typhoon paths\n",
    "\n",
    "# Integrate Quantum Computing:\n",
    "# Use Quantum Algorithms to Optimize the Predictive Model:\n",
    "\n",
    "# Utilize Qiskit for quantum optimization tasks.\n",
    "# Quantum Support Vector Machine (QSVM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090fe789-0f92-4581-92e7-5010e138e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_data['sst_gradient'] = combined_data['sst'].differentiate('lat') + combined_data['sst'].differentiate('lon')\n",
    "# combined_data['pressure_diff'] = combined_data['pressure'].differentiate('time')\n",
    "\n",
    "\n",
    "\n",
    "# from qiskit import Aer\n",
    "# from qiskit_machine_learning.algorithms import QSVM\n",
    "# from qiskit_machine_learning.kernels import QuantumKernel\n",
    "# from qiskit.circuit.library import ZZFeatureMap\n",
    "\n",
    "# # Define feature map and quantum kernel\n",
    "# feature_map = ZZFeatureMap(feature_dimension=3, reps=2, entanglement='linear')\n",
    "# quantum_kernel = QuantumKernel(feature_map=feature_map, quantum_instance=Aer.get_backend('qasm_simulator'))\n",
    "\n",
    "# # Define and train QSVM\n",
    "# qsvm = QSVM(quantum_kernel, X_train, y_train, X_test, y_test)\n",
    "# qsvm_results = qsvm.run()\n",
    "# print(qsvm_results['testing_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ebf89-6ea6-431b-81c3-85009ecb76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leverage Quantum Machine Learning Techniques:\n",
    "\n",
    "# Explore quantum-enhanced neural networks and hybrid classical-quantum models.\n",
    "# Variational Quantum Classifier (VQC):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6fb3cb-c63e-4f85-8c13-c185048bf0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qiskit.circuit.library import TwoLocal\n",
    "# from qiskit_machine_learning.algorithms.classifiers import VQC\n",
    "# from qiskit.utils import QuantumInstance\n",
    "\n",
    "# # Define variational form and quantum instance\n",
    "# var_form = TwoLocal(num_qubits=3, reps=2, rotation_blocks='ry', entanglement_blocks='cz')\n",
    "# quantum_instance = QuantumInstance(Aer.get_backend('qasm_simulator'), shots=1024)\n",
    "\n",
    "# # Define and train VQC\n",
    "# vqc = VQC(var_form, optimizer='COBYLA', feature_map=feature_map, quantum_instance=quantum_instance)\n",
    "# vqc.fit(X_train, y_train)\n",
    "# vqc_accuracy = vqc.score(X_test, y_test)\n",
    "# print(vqc_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9d3fb-00c1-4b62-9802-4f35afbe0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the Model:\n",
    "# Create a Real-Time Prediction System:\n",
    "\n",
    "# Develop a pipeline to process incoming climate data and generate predictions.\n",
    "# Example: Flask API for real-time predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7478f-4806-4083-a890-efe403b3f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, request, jsonify\n",
    "# import joblib\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# model = joblib.load('trained_model.pkl')\n",
    "\n",
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     data = request.json\n",
    "#     prediction = model.predict([data['features']])\n",
    "#     return jsonify({'prediction': prediction[0]})\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691fb2b5-8ffc-4deb-9272-f99b77381b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop a Visualization Dashboard:\n",
    "\n",
    "# Use tools like Dash or Plotly to create interactive visualizations.\n",
    "# Dash Dashboard for Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66126ccc-fa47-4019-bc9d-00e0cf6aae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dash\n",
    "# import dash_core_components as dcc\n",
    "# import dash_html_components as html\n",
    "# import plotly.express as px\n",
    "\n",
    "# app = dash.Dash(__name__)\n",
    "\n",
    "# app.layout = html.Div([\n",
    "#     dcc.Graph(id='sst-plot'),\n",
    "#     dcc.Interval(id='interval-component', interval=1*1000, n_intervals=0)\n",
    "# ])\n",
    "\n",
    "# @app.callback(\n",
    "#     dash.dependencies.Output('sst-plot', 'figure'),\n",
    "#     [dash.dependencies.Input('interval-component', 'n_intervals')]\n",
    "# )\n",
    "# def update_graph(n):\n",
    "#     fig = px.imshow(sst_data.mean(dim='time').values, labels=dict(color='SST Anomalies'))\n",
    "#     return fig\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a322ea0-f612-4f6a-9eb2-5c9f35e49636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #break\n",
    "# print('stop here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7b1166-90ce-4156-b6a3-f2d211365518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #break\n",
    "# print('stop here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdbd194-b903-4593-b076-d96d66dcfb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Simulation and Analysis:\n",
    "\n",
    "# Run quantum simulations to test the algorithm.\n",
    "# Analyze results using AI techniques to identify patterns and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7e77a-0f97-47e2-a146-68e7491ab58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Analyze the results of the quantum circuit\n",
    "# def analyze_quantum_results(counts):\n",
    "#     total_shots = sum(counts.values())\n",
    "#     probabilities = {state: count / total_shots for state, count in counts.items()}\n",
    "#     return probabilities\n",
    "\n",
    "# quantum_probabilities = analyze_quantum_results(quantum_results)\n",
    "# print(\"Quantum State Probabilities:\", quantum_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ecc72-74f4-44af-bbbf-86aa3362744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Validation:\n",
    "\n",
    "# Compare hybrid model outcomes with traditional models.\n",
    "# Validate the model using historical typhoon data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d69f7b-64a1-4882-b7df-c07149506e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validate the AI model\n",
    "# historical_data_url = \"https://example.com/historical_typhoon_data.csv\"\n",
    "# historical_data = pd.read_csv(historical_data_url)\n",
    "# historical_data_normalized = (historical_data - historical_data.mean()) / historical_data.std()\n",
    "\n",
    "# # Predict using the trained model\n",
    "# historical_X = historical_data_normalized.drop('target_variable', axis=1)\n",
    "# historical_y = historical_data_normalized['target_variable']\n",
    "# historical_y_pred = model.predict(historical_X)\n",
    "\n",
    "# # Evaluate the predictions\n",
    "# historical_mse = mean_squared_error(historical_y, historical_y_pred)\n",
    "# print(\"Historical Data Mean Squared Error:\", historical_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9846ec-dc32-47f2-8c94-ef87a07834a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Insight Generation:\n",
    "\n",
    "# Generate insights on typhoon impact and propose mitigation strategies.\n",
    "# Provide data-driven recommendations for policy and decision-making.\n",
    "# Deliverables\n",
    "# Code Repository: Public GitHub repository containing all code, data, and documentation.\n",
    "# Project Report: Comprehensive summary of the project, methodology, results, and conclusions.\n",
    "# Presentation Deck: 5-minute presentation summarizing the project outcomes.\n",
    "# Documentation: Detailed documentation of the algorithm, data sources, and analysis.\n",
    "# Next Steps\n",
    "# Complete Literature Review and Data Collection.\n",
    "# Develop Hybrid Algorithms with Qiskit and AI Models.\n",
    "# Run Simulations and Analyze Results.\n",
    "# Validate Models and Generate Insights.\n",
    "# Prepare Deliverables for Submission.\n",
    "# Additional Tips for Research and Development:\n",
    "# Research Resources:\n",
    "\n",
    "# Google Scholar: Google Scholar\n",
    "# Semantic Scholar: Semantic Scholar\n",
    "# ArXiv: ArXiv\n",
    "# Q4Climate: Q4Climate\n",
    "# Datasets, Models, Software:\n",
    "\n",
    "# NCAR Data Archive: NCAR Data Archive\n",
    "# NASA Earth Data: NASA Earth Data\n",
    "# Data.gov: Data.gov\n",
    "# Kaggle: Kaggle Datasets\n",
    "# OpenDAC: OpenDAC\n",
    "# Nvidia Modulus: Nvidia Modulus\n",
    "# Community Earth Systems Model: CESM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
